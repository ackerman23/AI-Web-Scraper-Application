{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Name not found', 'Affiliation': 'Affiliation not found', 'Research Interests': 'No interests found', 'Publications': [{'Title': 'Identifying drug–target interactions based on graph convolutional network and deep neural network', 'Year': '2021'}, {'Title': 'A learning-based framework for miRNA-disease association identification using neural networks', 'Year': '2019'}, {'Title': 'SemFunSim: a new method for measuring disease similarity by integrating semantic and gene functional association', 'Year': '2014'}, {'Title': 'LncRNA2Function: a comprehensive resource for functional investigation of human lncRNAs based on RNA-seq data', 'Year': '2015'}, {'Title': 'LncRNA2Target: a database for differentially expressed genes after lncRNA knockdown or overexpression', 'Year': '2015'}, {'Title': 'An end-to-end heterogeneous graph representation learning-based framework for drug–target interaction prediction', 'Year': '2021'}, {'Title': 'DeepLGP: a novel deep learning method for prioritizing lncRNA target genes', 'Year': '2020'}, {'Title': 'A learning-based method for drug-target interaction prediction based on feature representation learning and deep neural network', 'Year': '2020'}, {'Title': 'Genome-wide survival study identifies a novel synaptic locus and polygenic score for cognitive progression in Parkinson’s disease', 'Year': '2021'}, {'Title': \"Predicting Parkinson's disease genes based on node2vec and autoencoder\", 'Year': '2019'}, {'Title': 'InfAcrOnt: calculating cross-ontology term similarities using information flow by a random walk', 'Year': '2018'}, {'Title': 'Integrating multi-network topology for gene function prediction using deep neural networks', 'Year': '2021'}, {'Title': 'SC2disease: a manually curated database of single-cell transcriptome for human diseases', 'Year': '2021'}, {'Title': 'Deep learning‐based classification and mutation prediction from histopathological images of hepatocellular carcinoma', 'Year': '2020'}, {'Title': 'Prediction and collection of protein–metabolite interactions', 'Year': '2021'}, {'Title': 'Combining gene ontology with deep neural networks to enhance the clustering of single cell RNA-Seq data', 'Year': '2019'}, {'Title': 'Identifying term relations cross different gene ontology categories', 'Year': '2017'}, {'Title': 'Improving the measurement of semantic similarity by combining gene ontology and co-functional network: a random walk based approach', 'Year': '2018'}, {'Title': 'A novel method to measure the semantic similarity of HPO terms', 'Year': '2017'}, {'Title': 'Measuring semantic similarities by combining gene ontology annotations and gene co-function networks', 'Year': '2015'}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_google_scholar(profile_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(profile_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return {\"Error\": \"Unable to access the profile. Please check the URL.\"}\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract name\n",
    "    name_element = soup.find('h3', class_='gs_ai_name')\n",
    "    name = name_element.text.strip() if name_element else \"Name not found\"\n",
    "    \n",
    "    # Extract affiliation\n",
    "    affiliation_element = soup.find('div', class_='gs_ai_aff')\n",
    "    affiliation = affiliation_element.text.strip() if affiliation_element else \"Affiliation not found\"\n",
    "    \n",
    "    # Extract research interests (if available)\n",
    "    interests_element = soup.find('div', class_='gs_ai_int')\n",
    "    interests = interests_element.text.strip() if interests_element else \"No interests found\"\n",
    "    \n",
    "    # Extract publications\n",
    "    publications = []\n",
    "    for pub in soup.find_all('tr', class_='gsc_a_tr'):\n",
    "        title_element = pub.find('a', class_='gsc_a_at')\n",
    "        year_element = pub.find('span', class_='gsc_a_h')\n",
    "        title = title_element.text.strip() if title_element else \"No title\"\n",
    "        year = year_element.text.strip() if year_element else \"No year\"\n",
    "        publications.append({'Title': title, 'Year': year})\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"Affiliation\": affiliation,\n",
    "        \"Research Interests\": interests,\n",
    "        \"Publications\": publications\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "profile_url = \"https://scholar.google.com/citations?user=LZ1MiZwAAAAJ&hl=en\"  # Replace with the desired profile URL\n",
    "professor_info = scrape_google_scholar(profile_url)\n",
    "print(professor_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Academic Title</th>\n",
       "      <th>Email</th>\n",
       "      <th>Personal Homepage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chen Qun</td>\n",
       "      <td>Professor</td>\n",
       "      <td>chenbenben@nwpu.edu.cn</td>\n",
       "      <td>http://teacher.nwpu.edu.cn/en/1987000016.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Li Zhanhuai</td>\n",
       "      <td>Professor</td>\n",
       "      <td>lizhh@nwpu.edu.cn</td>\n",
       "      <td>http://teacher.nwpu.edu.cn/en/lizhanhuai.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shang Xuequn</td>\n",
       "      <td>Professor</td>\n",
       "      <td>shang@nwpu.edu.cn</td>\n",
       "      <td>http://teacher.nwpu.edu.cn/en/1983000017.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wu Jian</td>\n",
       "      <td>Professor</td>\n",
       "      <td>johnwu@nwpu.edu.cn</td>\n",
       "      <td>http://teacher.nwpu.edu.cn/en/2007010012.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chen Bolin</td>\n",
       "      <td>Associate Professor</td>\n",
       "      <td>blchen@nwpu.edu.cn</td>\n",
       "      <td>http://teacher.nwpu.edu.cn/en/2003000036.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name       Academic Title                   Email  \\\n",
       "0      Chen Qun            Professor  chenbenben@nwpu.edu.cn   \n",
       "1   Li Zhanhuai            Professor       lizhh@nwpu.edu.cn   \n",
       "2  Shang Xuequn            Professor       shang@nwpu.edu.cn   \n",
       "3       Wu Jian            Professor      johnwu@nwpu.edu.cn   \n",
       "4    Chen Bolin  Associate Professor      blchen@nwpu.edu.cn   \n",
       "\n",
       "                               Personal Homepage  \n",
       "0  http://teacher.nwpu.edu.cn/en/1987000016.html  \n",
       "1  http://teacher.nwpu.edu.cn/en/lizhanhuai.html  \n",
       "2  http://teacher.nwpu.edu.cn/en/1983000017.html  \n",
       "3  http://teacher.nwpu.edu.cn/en/2007010012.html  \n",
       "4  http://teacher.nwpu.edu.cn/en/2003000036.html  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"professors.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Failed to retrieve the page.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_google_scholar(profile_url):\n",
    "    response = requests.get(profile_url)\n",
    "    if response.status_code != 200:\n",
    "        return {\"error\": \"Failed to retrieve the page.\"}\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract name\n",
    "    name_tag = soup.find('h3', class_='gs_ai_name')\n",
    "    name = name_tag.text.strip() if name_tag else \"Name not found\"\n",
    "\n",
    "    # Extract affiliation\n",
    "    affiliation_tag = soup.find('div', class_='gs_ai_aff')\n",
    "    affiliation = affiliation_tag.text.strip() if affiliation_tag else \"Affiliation not found\"\n",
    "\n",
    "    # Extract research interests\n",
    "    interests_tag = soup.find('div', class_='gs_ai_int')\n",
    "    interests = interests_tag.text.strip() if interests_tag else \"No interests found\"\n",
    "\n",
    "    # Extract publications\n",
    "    publications = []\n",
    "    pub_tags = soup.find_all('tr', class_='gsc_a_tr')\n",
    "    for pub in pub_tags:\n",
    "        title_tag = pub.find('a', class_='gsc_a_at')\n",
    "        year_tag = pub.find('span', class_='gsc_a_h')\n",
    "        \n",
    "        title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "        year = year_tag.text.strip() if year_tag else \"Year not found\"\n",
    "        \n",
    "        publications.append({\"Title\": title, \"Year\": year})\n",
    "\n",
    "    return {\n",
    "        'Name': name,\n",
    "        'Affiliation': affiliation,\n",
    "        'Research Interests': interests,\n",
    "        'Publications': publications\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "profile_url = \"https://scholar.google.com/citations?user=LZ1MiZwAAAAJ&hl=en\"  # Replace with the desired profile URL\n",
    "professor_info = scrape_google_scholar(profile_url)\n",
    "print(professor_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Name not found', 'Affiliation': 'Affiliation not found', 'Research Interests': 'No interests found', 'Publications': [{'Title': 'Identifying drug–target interactions based on graph convolutional network and deep neural network', 'Year': '2021'}, {'Title': 'A learning-based framework for miRNA-disease association identification using neural networks', 'Year': '2019'}, {'Title': 'SemFunSim: a new method for measuring disease similarity by integrating semantic and gene functional association', 'Year': '2014'}, {'Title': 'LncRNA2Function: a comprehensive resource for functional investigation of human lncRNAs based on RNA-seq data', 'Year': '2015'}, {'Title': 'LncRNA2Target: a database for differentially expressed genes after lncRNA knockdown or overexpression', 'Year': '2015'}, {'Title': 'An end-to-end heterogeneous graph representation learning-based framework for drug–target interaction prediction', 'Year': '2021'}, {'Title': 'DeepLGP: a novel deep learning method for prioritizing lncRNA target genes', 'Year': '2020'}, {'Title': 'A learning-based method for drug-target interaction prediction based on feature representation learning and deep neural network', 'Year': '2020'}, {'Title': 'Genome-wide survival study identifies a novel synaptic locus and polygenic score for cognitive progression in Parkinson’s disease', 'Year': '2021'}, {'Title': \"Predicting Parkinson's disease genes based on node2vec and autoencoder\", 'Year': '2019'}, {'Title': 'InfAcrOnt: calculating cross-ontology term similarities using information flow by a random walk', 'Year': '2018'}, {'Title': 'Integrating multi-network topology for gene function prediction using deep neural networks', 'Year': '2021'}, {'Title': 'SC2disease: a manually curated database of single-cell transcriptome for human diseases', 'Year': '2021'}, {'Title': 'Deep learning‐based classification and mutation prediction from histopathological images of hepatocellular carcinoma', 'Year': '2020'}, {'Title': 'Prediction and collection of protein–metabolite interactions', 'Year': '2021'}, {'Title': 'Combining gene ontology with deep neural networks to enhance the clustering of single cell RNA-Seq data', 'Year': '2019'}, {'Title': 'Identifying term relations cross different gene ontology categories', 'Year': '2017'}, {'Title': 'Improving the measurement of semantic similarity by combining gene ontology and co-functional network: a random walk based approach', 'Year': '2018'}, {'Title': 'A novel method to measure the semantic similarity of HPO terms', 'Year': '2017'}, {'Title': 'Measuring semantic similarities by combining gene ontology annotations and gene co-function networks', 'Year': '2015'}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_google_scholar(profile_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(profile_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return {\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract name\n",
    "    name_tag = soup.find('h3', class_='gs_ai_name')\n",
    "    name = name_tag.text.strip() if name_tag else \"Name not found\"\n",
    "\n",
    "    # Extract affiliation\n",
    "    affiliation_tag = soup.find('div', class_='gs_ai_aff')\n",
    "    affiliation = affiliation_tag.text.strip() if affiliation_tag else \"Affiliation not found\"\n",
    "\n",
    "    # Extract research interests\n",
    "    interests_tag = soup.find('div', class_='gs_ai_int')\n",
    "    interests = interests_tag.text.strip() if interests_tag else \"No interests found\"\n",
    "\n",
    "    # Extract publications\n",
    "    publications = []\n",
    "    pub_tags = soup.find_all('tr', class_='gsc_a_tr')\n",
    "    for pub in pub_tags:\n",
    "        title_tag = pub.find('a', class_='gsc_a_at')\n",
    "        year_tag = pub.find('span', class_='gsc_a_h')\n",
    "        \n",
    "        title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "        year = year_tag.text.strip() if year_tag else \"Year not found\"\n",
    "        \n",
    "        publications.append({\"Title\": title, \"Year\": year})\n",
    "\n",
    "    return {\n",
    "        'Name': name,\n",
    "        'Affiliation': affiliation,\n",
    "        'Research Interests': interests,\n",
    "        'Publications': publications\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "profile_url = \"https://scholar.google.com/citations?user=LZ1MiZwAAAAJ&hl=en\"  # Replace with the desired profile URL\n",
    "professor_info = scrape_google_scholar(profile_url)\n",
    "print(professor_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/jihadgarti/miniforge3/envs/ai-scraper/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Using cached trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/jihadgarti/miniforge3/envs/ai-scraper/lib/python3.12/site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /Users/jihadgarti/miniforge3/envs/ai-scraper/lib/python3.12/site-packages (from selenium) (4.12.2)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/jihadgarti/miniforge3/envs/ai-scraper/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /Users/jihadgarti/miniforge3/envs/ai-scraper/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/jihadgarti/miniforge3/envs/ai-scraper/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Using cached selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
      "Using cached trio-0.26.2-py3-none-any.whl (475 kB)\n",
      "Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, sniffio, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.26.2 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m profile_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://scholar.google.com/citations?user=LZ1MiZwAAAAJ&hl=en\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Replace with the desired profile URL\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m professor_info \u001b[39m=\u001b[39m scrape_google_scholar_selenium(profile_url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(professor_info)\n",
      "\u001b[1;32m/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscrape_google_scholar_selenium\u001b[39m(profile_url):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Set up the Selenium WebDriver (you might need to specify the path to your ChromeDriver)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mChrome()  \u001b[39m# Or use webdriver.Firefox() for Firefox\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     driver\u001b[39m.\u001b[39mget(profile_url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihadgarti/Desktop/github-path/AI-Web-Scraper-Application/Teacher-data/exp.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Extract name\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[39m=\u001b[39m service \u001b[39mif\u001b[39;00m service \u001b[39melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[39m=\u001b[39m options \u001b[39mif\u001b[39;00m options \u001b[39melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     46\u001b[0m     browser_name\u001b[39m=\u001b[39;49mDesiredCapabilities\u001b[39m.\u001b[39;49mCHROME[\u001b[39m\"\u001b[39;49m\u001b[39mbrowserName\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     47\u001b[0m     vendor_prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgoog\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     48\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m     49\u001b[0m     service\u001b[39m=\u001b[39;49mservice,\n\u001b[1;32m     50\u001b[0m     keep_alive\u001b[39m=\u001b[39;49mkeep_alive,\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/site-packages/selenium/webdriver/chromium/webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice \u001b[39m=\u001b[39m service\n\u001b[1;32m     49\u001b[0m finder \u001b[39m=\u001b[39m DriverFinder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice, options)\n\u001b[0;32m---> 50\u001b[0m \u001b[39mif\u001b[39;00m finder\u001b[39m.\u001b[39;49mget_browser_path():\n\u001b[1;32m     51\u001b[0m     options\u001b[39m.\u001b[39mbinary_location \u001b[39m=\u001b[39m finder\u001b[39m.\u001b[39mget_browser_path()\n\u001b[1;32m     52\u001b[0m     options\u001b[39m.\u001b[39mbrowser_version \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/site-packages/selenium/webdriver/common/driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_browser_path\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_binary_paths()[\u001b[39m\"\u001b[39m\u001b[39mbrowser_path\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/site-packages/selenium/webdriver/common/driver_finder.py:67\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_paths[\u001b[39m\"\u001b[39m\u001b[39mdriver_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m path\n\u001b[1;32m     66\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     output \u001b[39m=\u001b[39m SeleniumManager()\u001b[39m.\u001b[39;49mbinary_paths(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_args())\n\u001b[1;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m Path(output[\u001b[39m\"\u001b[39m\u001b[39mdriver_path\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mis_file():\n\u001b[1;32m     69\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_paths[\u001b[39m\"\u001b[39m\u001b[39mdriver_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39mdriver_path\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/site-packages/selenium/webdriver/common/selenium_manager.py:55\u001b[0m, in \u001b[0;36mSeleniumManager.binary_paths\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     52\u001b[0m args\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m--output\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m args\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(args)\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/site-packages/selenium/webdriver/common/selenium_manager.py:119\u001b[0m, in \u001b[0;36mSeleniumManager._run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    117\u001b[0m     completed_proc \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mrun(args, capture_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, creationflags\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mCREATE_NO_WINDOW)\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     completed_proc \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mrun(args, capture_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    120\u001b[0m stdout \u001b[39m=\u001b[39m completed_proc\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m stderr \u001b[39m=\u001b[39m completed_proc\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39mpopenargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39;49mcommunicate(\u001b[39minput\u001b[39;49m, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    551\u001b[0m     \u001b[39mexcept\u001b[39;00m TimeoutExpired \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[39m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(\u001b[39minput\u001b[39;49m, endtime, timeout)\n\u001b[1;32m   1210\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[39m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(  \u001b[39m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfailed to raise TimeoutExpired.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m   2116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[39m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[39m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ai-scraper/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def scrape_google_scholar_selenium(profile_url):\n",
    "    # Set up the Selenium WebDriver (you might need to specify the path to your ChromeDriver)\n",
    "    driver = webdriver.Chrome()  # Or use webdriver.Firefox() for Firefox\n",
    "\n",
    "    driver.get(profile_url)\n",
    "\n",
    "    # Extract name\n",
    "    name = driver.find_element(By.CLASS_NAME, 'gs_ai_name').text.strip()\n",
    "\n",
    "    # Extract affiliation\n",
    "    affiliation = driver.find_element(By.CLASS_NAME, 'gs_ai_aff').text.strip()\n",
    "\n",
    "    # Extract research interests\n",
    "    interests = driver.find_element(By.CLASS_NAME, 'gs_ai_int').text.strip()\n",
    "\n",
    "    # Extract publications\n",
    "    publications = []\n",
    "    pub_tags = driver.find_elements(By.CLASS_NAME, 'gsc_a_tr')\n",
    "    for pub in pub_tags:\n",
    "        title = pub.find_element(By.CLASS_NAME, 'gsc_a_at').text.strip()\n",
    "        year = pub.find_element(By.CLASS_NAME, 'gsc_a_h').text.strip()\n",
    "        publications.append({\"Title\": title, \"Year\": year})\n",
    "\n",
    "    driver.quit()  # Close the browser\n",
    "    return {\n",
    "        'Name': name,\n",
    "        'Affiliation': affiliation,\n",
    "        'Research Interests': interests,\n",
    "        'Publications': publications\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "profile_url = \"https://scholar.google.com/citations?user=LZ1MiZwAAAAJ&hl=en\"  # Replace with the desired profile URL\n",
    "professor_info = scrape_google_scholar_selenium(profile_url)\n",
    "print(professor_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No profile found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_professor(professor_name):\n",
    "    # Google Scholar search URL for authors\n",
    "    search_url = f\"https://scholar.google.com/citations?view_op=search_authors&mauthors={professor_name.replace(' ', '+')}&hl=en\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return {\"error\": f\"Failed to retrieve search results. Status code: {response.status_code}\"}\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract profile link from the first result\n",
    "    profile_link = None\n",
    "    first_result = soup.find('a', class_='gsc_o_a')\n",
    "    if first_result and first_result['href']:\n",
    "        profile_link = \"https://scholar.google.com\" + first_result['href']  # Construct full URL\n",
    "\n",
    "    return profile_link\n",
    "\n",
    "def scrape_google_scholar(profile_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(profile_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return {\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract name\n",
    "    name_tag = soup.find('h3', class_='gs_ai_name')\n",
    "    name = name_tag.text.strip() if name_tag else \"Name not found\"\n",
    "\n",
    "    # Extract affiliation\n",
    "    affiliation_tag = soup.find('div', class_='gs_ai_aff')\n",
    "    affiliation = affiliation_tag.text.strip() if affiliation_tag else \"Affiliation not found\"\n",
    "\n",
    "    # Extract research interests\n",
    "    interests_tag = soup.find('div', class_='gs_ai_int')\n",
    "    interests = interests_tag.text.strip() if interests_tag else \"No interests found\"\n",
    "\n",
    "    # Extract publications\n",
    "    publications = []\n",
    "    pub_tags = soup.find_all('tr', class_='gsc_a_tr')\n",
    "    for pub in pub_tags:\n",
    "        title_tag = pub.find('a', class_='gsc_a_at')\n",
    "        year_tag = pub.find('span', class_='gsc_a_h')\n",
    "        \n",
    "        title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "        year = year_tag.text.strip() if year_tag else \"Year not found\"\n",
    "        \n",
    "        publications.append({\"Title\": title, \"Year\": year})\n",
    "\n",
    "    return {\n",
    "        'Name': name,\n",
    "        'Affiliation': affiliation,\n",
    "        'Research Interests': interests,\n",
    "        'Publications': publications\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "professor_name = \"Zhanhuai Li\"  # Replace with the desired professor's name\n",
    "profile_link = search_professor(professor_name)\n",
    "\n",
    "if profile_link:\n",
    "    professor_info = scrape_google_scholar(profile_link)\n",
    "    print(professor_info)\n",
    "else:\n",
    "    print(\"No profile found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
